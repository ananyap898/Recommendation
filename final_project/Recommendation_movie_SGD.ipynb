{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 612 Final project\n",
    "### Ananya\n",
    "### Introduction : \n",
    "This notebook spark implementation demonstrates movie recommendation using Stochastic Gradient Descent algorithm, which is another way to recommend a product or an item.\n",
    "Dataset is taken from the Movielens website : https://grouplens.org/datasets/movielens/.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is sgd\n",
    "\n",
    "1.loss function to be optimised \n",
    "\n",
    "2.weak learner to make prediction\n",
    "\n",
    "3.additive model to add weak learner to minimize the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required library \n",
    "import sys\n",
    "import math\n",
    "from time import time\n",
    "import random\n",
    "import csv\n",
    "import numpy\n",
    "from pyspark import SparkContext\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from functions import *\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This algorithm requires initialization of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\"rho\" is the decay factor or the exponentially weighted average over the square of the gradients.\n",
    "\n",
    "2.\"decay\" decays the learning rate over time, so we can move even closer to the local minimum in the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.2\n",
    "C = 6 # Number of factors\n",
    "number_of_iter = 50 # number of iterations\n",
    "block_number = 5 # number of blocks to take from the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function is an implementation of the SGD algorithm.\n",
    "This functions takes as an input a matrix R which is a sparse matrix, the mask matrix, and (Q, P), which are the factorisation of R.\n",
    "loss function to minimze the distance between R and QP\n",
    "Input : R, Q, P, mask, Ni, Nj, blockRange \n",
    "Output : Q, P, n, blockRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_SGD(R, Q, P, mask, no_row, no_column, blockRange):\n",
    "  \n",
    "    global rho\n",
    "    eta = .01 # first step size\n",
    "    R_new = R.nonzero()\n",
    "    n = R_new[0].size\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        j = random.randint(0, n-1) # Pick randomly an element j\n",
    "        row, col = R_new[0][j], R_new[1][j] # retrieve the row and column of the random j\n",
    "        \n",
    "        # take a small blocks from R, mask, Q and P\n",
    "        Ri = R[row,col] \n",
    "        maski = mask[row,col]\n",
    "        Qi = Q[row,:]\n",
    "        Pi = P[:,col]\n",
    "        \n",
    "        # compute the gradient of Qi and Pi\n",
    "        _, grad_Q = objective_Q(Pi, Qi, Ri, maski, rho/no_row[row])\n",
    "        _, grad_P = objective_P(Pi, Qi, Ri, maski, rho/no_column[col])\n",
    "        eta = eta * (1 + i) ** (- 0.5)\n",
    "        \n",
    "        # update the blocks of P and Q\n",
    "        Q[row,:] = Qi - eta * grad_Q\n",
    "        P[:,col] = Pi - eta * grad_P\n",
    "        \n",
    "        \n",
    "    return (Q, P, n, blockRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized SGD Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallelized_SGD(R, mask):\n",
    "    \n",
    "    \n",
    "    global nbr_iter, block_number, C\n",
    "    \n",
    "    Q = numpy.random.random_sample((R.shape[0], C))\n",
    "    P = numpy.random.random_sample((C, R.shape[1]))\n",
    "    block_i = (R.shape[0]/block_number, R.shape[1]/block_number)\n",
    "    \n",
    "    \n",
    "    rowRangeList = [[k*block_i[0],(k+1)*block_i[0]] for k in range(block_number)]\n",
    "    colRangeList = [[k*block_i[1],(k+1)*block_i[1]] for k in range(block_number)]\n",
    "\n",
    "    rowRangeList[-1][1] += R.shape[0]%block_number\n",
    "    colRangeList[-1][1] += R.shape[1]%block_number\n",
    "\n",
    "\n",
    "    for iter_ in range(number_of_iter):\n",
    "        if iter_ % 10 == 0:\n",
    "            print(\"... iteration %s\"%(iter_))\n",
    "        \n",
    "        for epoch in range(block_number):\n",
    "            grid = []\n",
    "            \n",
    "            for block in range(block_number):\n",
    "                rowRange = [int(rowRangeList[block][0]), int(rowRangeList[block][1])]\n",
    "                colRange = [int(colRangeList[block][0]), int(colRangeList[block][1])]\n",
    "                \n",
    "                Rn = R[rowRange[0]:rowRange[1], colRange[0]:colRange[1]]\n",
    "                maskn = mask[rowRange[0]:rowRange[1], colRange[0]:colRange[1]]\n",
    "                Qn = Q[rowRange[0]:rowRange[1],:]\n",
    "                Pn = P[:,colRange[0]:colRange[1]]\n",
    "                \n",
    "                no_row = {}\n",
    "                for i in range(rowRange[0],rowRange[1]):\n",
    "                    no_row[int(i-int(rowRange[0]))] = R[i,:].nonzero()[0].size\n",
    "                    \n",
    "                no_column = {}\n",
    "                for i in range(colRange[0],colRange[1]):\n",
    "                    no_column[i-colRange[0]] = R[:,i].nonzero()[0].size \n",
    "                    \n",
    "                if (Rn.nonzero()[0].size != 0):\n",
    "                    grid.append([Rn, Qn, Pn, maskn, no_row, no_column, (rowRange, colRange)])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            rdd = sc.parallelize(grid, block_number).\\\n",
    "                        map(lambda x: primary_SGD(x[0],x[1],x[2],x[3],x[4],x[5],x[6])).collect()\n",
    "                \n",
    "                \n",
    "            for elem in rdd:\n",
    "                rowRange,colRange = elem[3]\n",
    "                Q[rowRange[0]:rowRange[1],:] = elem[0]\n",
    "                P[:,colRange[0]:colRange[1]] = elem[1]\n",
    "\n",
    "            colRangeList.insert(0,colRangeList.pop())\n",
    "            \n",
    "            \n",
    "            \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function loads the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename=\"u.data\" , small_data=True):\n",
    "    \n",
    "    data = np.loadtxt(filename, dtype=int)\n",
    "    R = sparse.csr_matrix((data[:, 2], (data[:, 0]-1, data[:, 1]-1)),dtype=float)\n",
    "    mask = sparse.csr_matrix((np.ones(data[:, 2].shape),(data[:, 0]-1, data[:, 1]-1)), dtype=bool )\n",
    "    \n",
    "    if small_data is True:\n",
    "        R = (R[0:100, 0:100].copy())\n",
    "        mask = (mask[0:100, 0:100].copy())\n",
    "        \n",
    "        \n",
    "    return R.toarray(), mask.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This function returns :\n",
    "        R : the matrix user-item containing the ratings\n",
    "        mask : matrix is equal to 1 if a score existes and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark 2.3, this algorithm can be tested; SparkContext has been used to initialize the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... iteration 0\n",
      "... iteration 10\n",
      "... iteration 20\n",
      "... iteration 30\n",
      "... iteration 40\n"
     ]
    }
   ],
   "source": [
    "global R, P, Q\n",
    "spark = SparkContext.getOrCreate()\n",
    "R, mask = load_data(filename=\"u.data\" , small_data=True)\n",
    "Q, P = Parallelized_SGD(R, mask)\n",
    "rdd1 = spark.parallelize(P)\n",
    "df = rdd1.map(lambda x: x.tolist()).toDF([\"column\"])\n",
    "df.write.csv(\"outP.csv\")\n",
    "df.write.csv(\"outQ.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.80600252 3.22284498 1.5172746  ... 4.60971192 4.24265673 5.05350699]\n",
      " [3.31203728 2.94637395 0.89839108 ... 3.51413321 2.97226978 3.91307384]\n",
      " [2.88087961 2.10883337 1.03392691 ... 2.68440966 2.37356204 3.05356331]\n",
      " ...\n",
      " [3.60906029 2.41714227 1.16892304 ... 3.36043894 3.06578557 3.81572065]\n",
      " [3.86087734 2.48683463 1.25639892 ... 3.89525626 3.17046293 4.30298572]\n",
      " [3.01404063 1.81685796 0.90624342 ... 3.02373322 2.6698992  3.23433818]]\n"
     ]
    }
   ],
   "source": [
    "predicted_rating = Q.dot(P)\n",
    "print(predicted_rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the algorithm works relatively well since non zero values in R are very similar to those in QP at the same positions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error : 26.503981\n"
     ]
    }
   ],
   "source": [
    "relative_error = np.linalg.norm(mask*(R - np.dot(Q, P))) / np.linalg.norm(R) * 100\n",
    "print(\"Relative error : %f\"%relative_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.88218286,  5.9023236 ,  1.54953101,  6.12422833,  4.93079942,\n",
       "        5.94736754,  5.70524693,  7.40245003,  7.49506169,  4.48415911,\n",
       "        7.24541625,  9.01598888,  6.59359918,  5.74966485,  8.27202452,\n",
       "        4.91816549,  6.24388099,  5.08070523,  7.35784124,  6.10113013,\n",
       "        4.2225028 ,  9.82495594,  9.41721332,  6.83417951,  5.98669841,\n",
       "        5.18852988,  4.9094934 ,  7.3550913 ,  2.6789372 ,  7.90164413,\n",
       "        6.796131  ,  8.73526922,  3.43627418,  1.93866597,  5.8973791 ,\n",
       "        6.17257601,  3.24291813,  4.73460774,  5.42392883,  5.34247088,\n",
       "        5.04654079,  7.372278  ,  6.11757673,  8.57369832,  7.69972424,\n",
       "       11.46237997,  4.96680887,  8.20788964,  8.01276108,  5.53815808,\n",
       "        6.70658272,  7.97710276,  6.06108316,  5.1474098 ,  9.79355861,\n",
       "        7.16778199,  3.6479794 ,  6.44111422, 10.20204468,  6.84419061,\n",
       "        5.85149581,  6.86685578,  6.4027803 ,  8.6940243 ,  5.82537558,\n",
       "        6.30819354,  4.29829092,  4.49677041,  7.05938314,  4.60089923,\n",
       "        6.87696085,  3.3723675 ,  5.97680134,  3.94057226,  5.60719619,\n",
       "        5.76239322,  6.10796302,  4.55749558,  5.48428629,  3.8841496 ,\n",
       "        6.60361141,  5.53413394,  8.48190621,  6.90622813,  9.92676811,\n",
       "        7.06722412,  6.00480114,  7.41844852,  8.11934319,  3.78741369,\n",
       "        5.63072768,  5.59663951,  7.78640693,  4.33870397,  8.6475339 ,\n",
       "        8.08068812,  6.65900486,  6.39534716,  8.33155727,  6.57471861])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(R, predicted_rating, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the movie that we'll recommand to the user nÂ° u, we have to consider the decomposition $ R = QP$.\n",
    "\n",
    "In fact, by multiplying the $u^{th}$ row of Q by the matrix P, we obtain a vector r of size $ I$ x $ 1$ that contains all the estimated ratings of movies for this user u.   \n",
    "\n",
    "Now, we only need to consider the highest score and take its index (which correspond to the recommanded movie), however we shoudn't forget that we have to avoid recommanding a movie that the user had already seen, that's why we need to multiply our vector r by the opposite of the mask. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let us now recommend a user 45 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommand to the user  : 2 movie : 63\n",
      "The rating of this movie is :  3.1739291676863988\n"
     ]
    }
   ],
   "source": [
    "u = 2\n",
    "r = np.dot(Q[u,:], P)\n",
    "\n",
    "r = r * (1 - mask[u,:])\n",
    "\n",
    "movie_index = np.argmax(r)\n",
    "print(\"We recommand to the user  :\", u ,\"movie :\" ,movie_index)\n",
    "print (\"The rating of this movie is : \" ,r[movie_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommand to the user  : 44  movie : 63\n",
      "The rating of this movie is :  3.127435836829044\n"
     ]
    }
   ],
   "source": [
    "u = 44\n",
    "r = np.dot(Q[u,:], P)\n",
    "\n",
    "r = r * (1 - mask[u,:])\n",
    "\n",
    "movie_index = np.argmax(r)\n",
    "print(\"We recommand to the user  :\", u ,\" movie :\" ,movie_index)\n",
    "print (\"The rating of this movie is : \" ,r[movie_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary : Spark helped achieve parallel distribution while processing the datasets. The parallel STochastic Gradoent Descent algorithm distributed the computed matrix over many spark workers (block number) and then aggregate the result. The number of iterations can be customized and can help improve the performance. It is recommended that a very large set be leveraged in a cloud or any cluster setting environment in order to get a better recommendation result."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
